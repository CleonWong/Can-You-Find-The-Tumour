{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try using keras Functional API to build Unet\n",
    "\n",
    "For subclass model method of building, refer to `./UnetExperiment_Subclass.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "# from keras.backend.common import _EPSILON\n",
    "# from tensorflow.keras.backend import common \n",
    "\n",
    "# Misc\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "import jsonref\n",
    "import json\n",
    "\n",
    "# Save\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For custom weighted loss\n",
    "# from tensorflow.keras.backend import _to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20201205_153027'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 9.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "weights = [1, 9]\n",
    "\n",
    "K_weights = K.constant(weights)\n",
    "\n",
    "if not K.is_tensor(y_pred):\n",
    "    y_pred = K.constant(y_pred)\n",
    "\n",
    "# Cast y_true to the datatype of y_pred.\n",
    "y_true = K.cast(y_true, y_pred.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1.]\n",
      " [0. 1.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.2 0.9]\n",
      " [0.1 0.8]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "tf.Tensor([0.20067078 0.11778302], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(K.categorical_crossentropy(target=y_true, output=y_pred, from_logits=False, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.2 0.9]\n",
      " [0.1 0.8]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.1       ]\n",
      " [0.90000004]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.18181819 0.81818175]\n",
      " [0.11111111 0.8888889 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.704748   -0.20067078]\n",
      " [-2.1972246  -0.11778302]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor([0.20067078 0.11778302], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "denom = tf.reduce_sum(y_pred, axis=1, keepdims=True)\n",
    "print(denom)\n",
    "y_pred = y_pred / denom\n",
    "print(y_pred)\n",
    "print(tf.math.log(y_pred))\n",
    "print(- tf.reduce_sum(y_true * tf.math.log(y_pred), axis=len(y_pred.get_shape()) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pixelwise_crossentropy(class_weights):\n",
    "    \n",
    "    def compute_loss(y_true, y_pred):\n",
    "        # Clip y_pred to prevent log(0).\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = tf.clip_by_value(t=y_pred, clip_value_min=epsilon, clip_value_max=1. - epsilon)\n",
    "        \n",
    "        ce_matrix = tf.multiply(y_true, tf.math.log(y_pred))\n",
    "        weighted_ce_loss = - tf.reduce_sum(tf.multiply(ce_matrix, class_weights))\n",
    "\n",
    "        return weighted_ce_loss\n",
    "\n",
    "    return compute_loss\n",
    "\n",
    "def get_class_weights(y_true, class_0_weight, class_1_weight):\n",
    "    class_weights = np.where(y_true==0, class_0_weight, class_1_weight)\n",
    "    class_weights = K.constant(class_weights)\n",
    "    \n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 1.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[1. 0.]\n",
      "  [1. 1.]]], shape=(2, 2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.2 0.9]\n",
      "  [0.1 0.3]]\n",
      "\n",
      " [[0.9 0.1]\n",
      "  [0.1 0.2]]], shape=(2, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[[0, 1], [0, 0]], [[1, 0], [1, 1]]])\n",
    "y_pred = np.array([[[0.2, 0.9], [0.1, 0.3]], [[0.9, 0.1], [0.1, 0.2]]])\n",
    "\n",
    "y_true = K.constant(y_true)\n",
    "y_pred = K.constant(y_pred)\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.29565367, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class_weights = get_class_weights(y_true=y_true, class_0_weight=0.1, class_1_weight=0.9)\n",
    "compute_loss = weighted_pixelwise_crossentropy(class_weights)\n",
    "print(compute_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_img_dir = \"../data/preprocessed/Mass/Train_FULL\"\n",
    "train_mask_img_dir = \"../data/preprocessed/Mass/Train_MASK\"\n",
    "test_full_img_dir = \"../data/preprocessed/Mass/Test_FULL\"\n",
    "test_mask_img_dir = \"../data/preprocessed/Mass/Test_MASK\"\n",
    "extension = \".png\"\n",
    "valtest_split = 0.5\n",
    "\n",
    "train_full_img_paths = []\n",
    "train_mask_img_paths = []\n",
    "test_full_img_paths = []\n",
    "test_mask_img_paths = []\n",
    "\n",
    "# Get paths of train images and masks.\n",
    "for full in os.listdir(train_full_img_dir):\n",
    "    if full.endswith(extension):\n",
    "        train_full_img_paths.append(os.path.join(train_full_img_dir, full))\n",
    "\n",
    "for mask in os.listdir(test_full_img_dir):\n",
    "    if mask.endswith(extension):\n",
    "        train_mask_img_paths.append(os.path.join(train_mask_img_dir, mask))\n",
    "\n",
    "# Get paths of test images and masks.\n",
    "for full in os.listdir(test_full_img_dir):\n",
    "    if full.endswith(extension):\n",
    "        test_full_img_paths.append(os.path.join(test_full_img_dir, full))\n",
    "\n",
    "for mask in os.listdir(test_mask_img_dir):\n",
    "    if mask.endswith(extension):\n",
    "        test_mask_img_paths.append(os.path.join(test_mask_img_dir, mask))\n",
    "\n",
    "# Sort so that FULL and MASK images are in an order that corresponds\n",
    "# with each other.\n",
    "train_full_img_paths.sort()\n",
    "train_mask_img_paths.sort()\n",
    "test_full_img_paths.sort()\n",
    "test_mask_img_paths.sort()\n",
    "\n",
    "# Split test paths into validation and test sets.\n",
    "valid_x, test_x = train_test_split(test_full_img_paths, test_size=valtest_split, random_state=42)\n",
    "valid_y, test_y = train_test_split(test_mask_img_paths, test_size=valtest_split, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 2, 3)\n",
      "[[[1 1 1]\n",
      "  [2 2 2]]\n",
      "\n",
      " [[3 3 3]\n",
      "  [4 4 4]]]\n"
     ]
    }
   ],
   "source": [
    "img = np.array([[1, 2], [3, 4]])\n",
    "print(img.shape)\n",
    "stacked_img = np.stack([img, img, img], axis=-1)\n",
    "print(stacked_img.shape)\n",
    "print(stacked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder():\n",
    "        \n",
    "        # =========\n",
    "        #  Encoder\n",
    "        # =========\n",
    "        \n",
    "        # Get base model\n",
    "        VGG16_ = keras.applications.VGG16(include_top=False,\n",
    "                                          weights=\"imagenet\",\n",
    "                                          input_shape=self.encoder_input_shape)\n",
    "        \n",
    "        # Get list of layer names for skip connections later\n",
    "        layer_names = [layer.name for layer in VGG16_.layers]\n",
    "\n",
    "        # Get layer outputs\n",
    "        all_layer_outputs = [VGG16_.get_layer(layer_name).output for layer_name in layer_names]\n",
    "        \n",
    "        # Create encoder model\n",
    "        encoder_model = keras.Model(inputs=VGG16_.input, outputs=all_layer_outputs)\n",
    "        \n",
    "        # Freeze layers\n",
    "        encoder_model.trainable = False\n",
    "        \n",
    "        return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetVGG16:\n",
    "    \n",
    "    def __init__(self, encoder_input_shape, learning_rate, batch_size, kernel_size, decoder_strides, decoder_padding, decoder_activation):\n",
    "        \n",
    "        self.encoder_input_shape = encoder_input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.decoder_strides = decoder_strides\n",
    "        self.decoder_padding = decoder_padding\n",
    "        self.decoder_activation = decoder_activation\n",
    "        \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \n",
    "        # =========\n",
    "        #  Encoder\n",
    "        # =========\n",
    "        \n",
    "        # Get base model\n",
    "        VGG16_ = keras.applications.VGG16(include_top=False,\n",
    "                                          weights=\"imagenet\",\n",
    "                                          input_shape=self.encoder_input_shape)\n",
    "        \n",
    "        # Get list of layer names for skip connections later\n",
    "        layer_names = [layer.name for layer in VGG16_.layers]\n",
    "\n",
    "        # Get layer outputs\n",
    "        all_layer_outputs = [VGG16_.get_layer(layer_name).output for layer_name in layer_names]\n",
    "        \n",
    "        # Create encoder model\n",
    "        encoder_model = keras.Model(inputs=VGG16_.input, outputs=all_layer_outputs)\n",
    "        \n",
    "        # Freeze layers\n",
    "        encoder_model.trainable = False\n",
    "        \n",
    "        return encoder_model\n",
    "        \n",
    "    \n",
    "    def build_unet(self):\n",
    "        \n",
    "        # =============\n",
    "        #  Input layer\n",
    "        # =============\n",
    "        \n",
    "        unet_input = keras.Input(shape=self.encoder_input_shape, name=\"unet_input_layer\")\n",
    "        \n",
    "        \n",
    "        # =========\n",
    "        #  Encoder\n",
    "        # =========\n",
    "        \n",
    "        encoder_model = self.build_encoder()\n",
    "        all_encoder_layer_outputs = encoder_model(unet_input)\n",
    "        \n",
    "        # Get final encoder output (this will be the input for the decoder)\n",
    "        encoded_img = all_encoder_layer_outputs[-1]\n",
    "        \n",
    "        # Get outputs to be used for skip connections\n",
    "        # (I know the specific layers to be used for skip connections)\n",
    "        skip_outputs = [all_encoder_layer_outputs[i] for i in [2, 5, 9, 13, 17]]\n",
    "        \n",
    "        \n",
    "        # =========\n",
    "        #  Decoder\n",
    "        # =========\n",
    "        \n",
    "        decoder_filters = encoded_img.shape[-1]\n",
    "        \n",
    "#         # Input layer into decoder\n",
    "#         decoder_input = keras.Input(shape=encoded_img.shape[1:], name=\"encoded_img\")\n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 5 - `encoded_img` as initial input for decoder\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block5_up_convT\",\n",
    "                                          filters=decoder_filters,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(encoded_img)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block5_up_concat\", axis=-1)([x, skip_outputs[4]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block5_up_conv3\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block5_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block5_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 4\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block4_up_convT\",\n",
    "                                          filters=decoder_filters,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block4_up_concat\", axis=-1)([x, skip_outputs[3]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block4_up_conv3\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block4_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block4_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 3\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block3_up_convT\",\n",
    "                                          filters=decoder_filters / 2,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block3_up_concat\", axis=-1)([x, skip_outputs[2]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block3_up_conv3\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block3_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block3_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 2\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block2_up_convT\",\n",
    "                                          filters=decoder_filters / 4,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block2_up_concat\", axis=-1)([x, skip_outputs[1]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block2_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block2_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 1\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block1_up_convT\",\n",
    "                                          filters=decoder_filters / 4,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block1_up_concat\", axis=-1)([x, skip_outputs[0]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block1_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block1_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Final conv layer\n",
    "        final_img = keras.layers.Conv2D(name=\"final_up_conv\",\n",
    "                                        filters=3,\n",
    "                                        kernel_size=self.kernel_size,\n",
    "                                        strides=(1, 1),\n",
    "                                        padding=\"same\",\n",
    "                                        activation=\"relu\")(x)\n",
    "        \n",
    "        # ======\n",
    "        #  Unet\n",
    "        # ======\n",
    "        \n",
    "        unet = keras.Model(inputs=unet_input, outputs=final_img, name=\"Unet_VGG16\")\n",
    "        \n",
    "        return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UnetVGG16(encoder_input_shape=(448, 448, 3),\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=1,\n",
    "                 kernel_size=(3, 3),\n",
    "                 decoder_strides=(2, 2),\n",
    "                 decoder_padding=\"same\",\n",
    "                 decoder_activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = unet.build_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet_VGG16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "unet_input_layer (InputLayer)   [(None, 448, 448, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_16 (Functional)      [(None, 448, 448, 3) 14714688    unet_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_convT (Conv2DTranspos (None, 28, 28, 512)  2359808     functional_16[0][18]             \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_concat (Concatenate)  (None, 28, 28, 1024) 0           block5_up_convT[0][0]            \n",
      "                                                                 functional_16[0][17]             \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_conv3 (Conv2D)        (None, 28, 28, 512)  4719104     block5_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_conv2 (Conv2D)        (None, 28, 28, 512)  2359808     block5_up_conv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_conv1 (Conv2D)        (None, 28, 28, 512)  2359808     block5_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_convT (Conv2DTranspos (None, 56, 56, 512)  2359808     block5_up_conv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_concat (Concatenate)  (None, 56, 56, 1024) 0           block4_up_convT[0][0]            \n",
      "                                                                 functional_16[0][13]             \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_conv3 (Conv2D)        (None, 56, 56, 512)  4719104     block4_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_conv2 (Conv2D)        (None, 56, 56, 512)  2359808     block4_up_conv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_conv1 (Conv2D)        (None, 56, 56, 512)  2359808     block4_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_convT (Conv2DTranspos (None, 112, 112, 256 1179904     block4_up_conv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_concat (Concatenate)  (None, 112, 112, 512 0           block3_up_convT[0][0]            \n",
      "                                                                 functional_16[0][9]              \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_conv3 (Conv2D)        (None, 112, 112, 512 2359808     block3_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_conv2 (Conv2D)        (None, 112, 112, 512 2359808     block3_up_conv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_conv1 (Conv2D)        (None, 112, 112, 512 2359808     block3_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_up_convT (Conv2DTranspos (None, 224, 224, 128 589952      block3_up_conv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_up_concat (Concatenate)  (None, 224, 224, 256 0           block2_up_convT[0][0]            \n",
      "                                                                 functional_16[0][5]              \n",
      "__________________________________________________________________________________________________\n",
      "block2_up_conv2 (Conv2D)        (None, 224, 224, 512 1180160     block2_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_up_conv1 (Conv2D)        (None, 224, 224, 512 2359808     block2_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_up_convT (Conv2DTranspos (None, 448, 448, 128 589952      block2_up_conv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_up_concat (Concatenate)  (None, 448, 448, 192 0           block1_up_convT[0][0]            \n",
      "                                                                 functional_16[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "block1_up_conv2 (Conv2D)        (None, 448, 448, 512 885248      block1_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_up_conv1 (Conv2D)        (None, 448, 448, 512 2359808     block1_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_up_conv (Conv2D)          (None, 448, 448, 3)  13827       block1_up_conv1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 54,549,827\n",
      "Trainable params: 39,835,139\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(unet_model, to_file=\"./experimental_outputs/unet_model_v1.png\", show_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = [np.random.uniform(low=0, high=1.0, size=(448, 448, 3)) for i in range(10)]\n",
    "y_train = [np.random.randint(low=0, high=2, size=(448, 448, 3)) for i in range(10)]\n",
    "\n",
    "def GenerateInputs(X, y):\n",
    "    for i in range(len(X)):\n",
    "        X_input = X[i].reshape(1, 448, 448, 3)\n",
    "        y_input = y[i].reshape(1, 448, 448, 3)\n",
    "        yield (X_input,y_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                   loss=\"binary_crossentropy\",\n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 - 12s - loss: 2.5556 - accuracy: 0.3837\n",
      "Epoch 2/2\n",
      "5/5 - 13s - loss: 2.1758 - accuracy: 0.3576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x152ba6130>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_model.fit(GenerateInputs(X=x_train, y=y_train), batch_size=1, steps_per_epoch = 5, epochs=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x152ba61c0>\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x153182a30>\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x153229880>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x153229a60>\n"
     ]
    }
   ],
   "source": [
    "for l in unet_model.layers:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
